{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ada8a6",
   "metadata": {},
   "source": [
    "## Use Case and Better Evalution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833a05f",
   "metadata": {},
   "source": [
    "1. Load in tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60b85c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"./bert-fake-news\")\n",
    "model.eval()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3fdcc7",
   "metadata": {},
   "source": [
    "2. Interpret results using LIME, output html file with explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef200bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Real (99.90% confidence)\n",
      "Explanation saved to lime_explanation.html.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "import webbrowser\n",
    "\n",
    "class_names = [\"Fake\", \"Real\"]\n",
    "\n",
    "explainer = LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "sample_text = \"Trump launches nuclear warhead on Iran and starts world war 3.\"\n",
    "\n",
    "def predict_proba(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
    "    return probs\n",
    "\n",
    "probs = predict_proba([sample_text])[0]\n",
    "predicted_class = np.argmax(probs)\n",
    "confidence = probs[predicted_class]\n",
    "\n",
    "print(f\"Prediction: {class_names[predicted_class]} ({confidence*100:.2f}% confidence)\")\n",
    "\n",
    "explanation = explainer.explain_instance(\n",
    "    sample_text,\n",
    "    predict_proba,\n",
    "    num_features=10,\n",
    "    labels=(predicted_class,)\n",
    ")\n",
    "\n",
    "with open(\"lime_explanation.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(explanation.as_html())\n",
    "\n",
    "print(\"Explanation saved to lime_explanation.html.\")\n",
    "webbrowser.open(\"lime_explanation.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ce25c8",
   "metadata": {},
   "source": [
    "3. Set up prompt for gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc9193be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lime_text = explanation.as_list(label=predicted_class)\n",
    "\n",
    "explanation_summary = \"\\n\".join(\n",
    "    f\"The word '{word}' contributed {weight:.2f} to the prediction.\"\n",
    "    for word, weight in lime_text\n",
    ")\n",
    "\n",
    "llm_prompt = f\"\"\"\n",
    "The model classified the article as '{class_names[predicted_class]}' with {confidence*100:.1f}% confidence.\n",
    "\n",
    "Here are the top contributing words from the model's explanation:\n",
    "\n",
    "{explanation_summary}\n",
    "\n",
    "Can you explain in plain English why the model might have made this decision?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4f29fc",
   "metadata": {},
   "source": [
    "4. Use gemini to transfer model results and LIME explanation to human readable text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a2bb03ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This news article appears to be real because it contains a mix of words often found in factual reporting about international relations and events. The presence of terms like \"warhead,\" \"nuclear,\" and \"Iran,\" combined with references to leaders like \"Trump,\" suggests the article discusses potential geopolitical issues in a serious and grounded manner. While words like \"launches\" and \"starts\" might sometimes appear in sensationalized headlines, their impact is offset by the other terms that lend credibility to the article's overall tone. The inclusion of specific numbers like \"3\" also hints at a level of detail characteristic of genuine news reports.\n",
      "\n",
      "Confidence: 99.9%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "\n",
    "response = model.generate_content([\n",
    "    \"You are an AI assistant that explains the results of a machine learning system which decides whether a news article is fake or real. \"\n",
    "    \"Explain the results clearly and simply, so that anyone without technical knowledge can understand. \"\n",
    "    \"Avoid mentioning the AI model or technical details. Do not explain the weights, write the text as if its an authority and not the model thinks this the model thinks that.\"\n",
    "    \"Add the confidence percentage at the end, this is very important\"\n",
    "    \"Focus on helping people grasp why the system made its decision.\"\n",
    "    \"I don't want lists of the words, I want the explanation only!\",\n",
    "    llm_prompt\n",
    "])\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
